{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyarrow\n",
    "!pip install --upgrade pandas\n",
    "from google.colab import drive\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/content/drive/'            # This is how you get to the \"root\" folder of your google drive. \n",
    "BASE = 'My Drive/Umar - Omdena Newsroom /' # This is where you specify the subfolder that is the working folder for this notebook. \n",
    "PROJECT_PATH = join(ROOT,BASE)\n",
    "\n",
    "drive.mount(ROOT)\n",
    "%cd '{PROJECT_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(PROJECT_PATH + 'data/omdena_master.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabelled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the feather files into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the unlabelled dataset provided by Omdena. These are the 13 or so feather files. Each will be loaded up and added to a dataframe. The end result (df_unlab) is a dataset with all the articles in one frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the unlabbeled data from the Feather files. \n",
    "\n",
    "unlab_data_path = PROJECT_PATH + 'data/raw_unlabeled/'\n",
    "\n",
    "df_unlab=pd.DataFrame()\n",
    "for file in os.listdir(unlab_data_path):\n",
    "    df_tmp = pd.read_feather(unlab_data_path + file)\n",
    "    df_unlab = df_unlab.append(df_tmp)\n",
    "\n",
    "df_unlab.reset_index(inplace=True, drop=True)\n",
    "df_unlab.to_csv(PROJECT_PATH + 'data/raw_combined_articles.csv')\n",
    "#df_unlab.to_sql('raw_dataset',conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nytimes                    7148\n",
       "cnn                        7148\n",
       "breitbart                  7148\n",
       "bbc                        7148\n",
       "nypost                     7147\n",
       "timesofindia.indiatimes    7147\n",
       "foxnews                    7147\n",
       "chicagotribune             7147\n",
       "reuters                    7147\n",
       "aljazeera                  7147\n",
       "cbsnews                    7147\n",
       "businessinsider            7147\n",
       "nbcnews                    7147\n",
       "latimes                    7147\n",
       "nationalreview             7147\n",
       "newsweek                   7147\n",
       "theguardian                7146\n",
       "thesun                     7146\n",
       "thehill                    7146\n",
       "telegraph                  7146\n",
       "independent                7146\n",
       "vox                        7146\n",
       "vice                       7146\n",
       "politico                   7146\n",
       "washingtontimes            7146\n",
       "dailymail                  7146\n",
       "boingboing                 7143\n",
       "usatoday                   7141\n",
       "apnews                     7096\n",
       "dailycaller                6604\n",
       "wired                      6146\n",
       "zerohedge                  5385\n",
       "theblaze                   4493\n",
       "realclearpolitics          4464\n",
       "observer                   4008\n",
       "thegatewaypundit           3512\n",
       "americanthinker            2242\n",
       "newsbusters                1396\n",
       "theamericanconservative    1132\n",
       "yahoo                       325\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean URLS to extract source\n",
    "\n",
    "def strip_html (html_string):\n",
    "    text = html_string[1:-1]\n",
    "    text = bs(text).get_text()\n",
    "    return text\n",
    "\n",
    "def strip_url(string_in,suffix):\n",
    "    url_string = string_in\n",
    "    for suf in suffix:\n",
    "        if suf in url_string:\n",
    "            pos = url_string.find(suf)\n",
    "            url_string = url_string[:pos]\n",
    "    \n",
    "    url_string = url_string.replace('https://','')\n",
    "    url_string = url_string.replace('http://','')\n",
    "    url_string = url_string.replace('www.','')\n",
    "    return url_string\n",
    "\n",
    "df_unlab['title'] = df_unlab['article_title'].apply(strip_html,1)\n",
    "df_unlab['subtitle'] = df_unlab['article_subtitle'].apply(strip_html,1)\n",
    "df_unlab['text'] = df_unlab['article_text'].apply(strip_html,1)\n",
    "df_unlab['author'] = df_unlab['author_name'].apply(strip_html,1)\n",
    "\n",
    "suffs = ['.com','.org','.co.uk','.net']\n",
    "df_unlab['source'] = df_unlab['link'].apply(strip_url,1,args=(suffs,))\n",
    "\n",
    "df_unlab.drop(['article_title','article_subtitle','article_text','author_name'], axis =1, inplace=True)\n",
    "\n",
    "df_unlab.reset_index()\n",
    "df_unlab.to_csv(PROJECT_PATH + 'data/cleaned_unlab.csv')\n",
    "df_unlab.to_sql('cleaned',conn,if_exists='replace')\n",
    "conn.commit()\n",
    "\n",
    "df_unlab.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
