{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade pyarrow\n",
    "!pip install --upgrade pandas\n",
    "from google.colab import drive\n",
    "from os.path import join\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "from nltk.stem.porter import *\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "import string\n",
    "import re\n",
    "from nltk.stem.porter import *\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = '/content/drive/'            # This is how you get to the \"root\" folder of your google drive. \n",
    "BASE = 'My Drive/Umar - Omdena Newsroom /' # This is where you specify the subfolder that is the working folder for this notebook. \n",
    "PROJECT_PATH = join(ROOT,BASE)\n",
    "\n",
    "drive.mount(ROOT)\n",
    "%cd '{PROJECT_PATH}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = sqlite3.connect(PROJECT_PATH + 'data/omdena_master.db')\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
    "print(cursor.fetchall())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unlabelled Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the feather files into one dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will load the unlabelled dataset provided by Omdena. These are the 13 or so feather files. Each will be loaded up and added to a dataframe. The end result (df_unlab) is a dataset with all the articles in one frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the unlabbeled data from the Feather files. \n",
    "\n",
    "unlab_data_path = PROJECT_PATH + 'data/raw_unlabeled/'\n",
    "\n",
    "df_unlab=pd.DataFrame()\n",
    "for file in os.listdir(unlab_data_path):\n",
    "    df_tmp = pd.read_feather(unlab_data_path + file)\n",
    "    df_unlab = df_unlab.append(df_tmp)\n",
    "\n",
    "df_unlab.reset_index(inplace=True, drop=True)\n",
    "df_unlab.to_csv(PROJECT_PATH + 'data/raw_combined_articles.csv')\n",
    "#df_unlab.to_sql('raw_dataset',conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract text from HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "nytimes                    7148\n",
       "cnn                        7148\n",
       "breitbart                  7148\n",
       "bbc                        7148\n",
       "nypost                     7147\n",
       "timesofindia.indiatimes    7147\n",
       "foxnews                    7147\n",
       "chicagotribune             7147\n",
       "reuters                    7147\n",
       "aljazeera                  7147\n",
       "cbsnews                    7147\n",
       "businessinsider            7147\n",
       "nbcnews                    7147\n",
       "latimes                    7147\n",
       "nationalreview             7147\n",
       "newsweek                   7147\n",
       "theguardian                7146\n",
       "thesun                     7146\n",
       "thehill                    7146\n",
       "telegraph                  7146\n",
       "independent                7146\n",
       "vox                        7146\n",
       "vice                       7146\n",
       "politico                   7146\n",
       "washingtontimes            7146\n",
       "dailymail                  7146\n",
       "boingboing                 7143\n",
       "usatoday                   7141\n",
       "apnews                     7096\n",
       "dailycaller                6604\n",
       "wired                      6146\n",
       "zerohedge                  5385\n",
       "theblaze                   4493\n",
       "realclearpolitics          4464\n",
       "observer                   4008\n",
       "thegatewaypundit           3512\n",
       "americanthinker            2242\n",
       "newsbusters                1396\n",
       "theamericanconservative    1132\n",
       "yahoo                       325\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean URLS to extract source\n",
    "\n",
    "def strip_html (html_string):\n",
    "    text = html_string[1:-1]\n",
    "    text = bs(text).get_text()\n",
    "    return text\n",
    "\n",
    "def strip_url(string_in,suffix):\n",
    "    url_string = string_in\n",
    "    for suf in suffix:\n",
    "        if suf in url_string:\n",
    "            pos = url_string.find(suf)\n",
    "            url_string = url_string[:pos]\n",
    "    \n",
    "    url_string = url_string.replace('https://','')\n",
    "    url_string = url_string.replace('http://','')\n",
    "    url_string = url_string.replace('www.','')\n",
    "    return url_string\n",
    "\n",
    "df_unlab['title'] = df_unlab['article_title'].apply(strip_html,1)\n",
    "df_unlab['subtitle'] = df_unlab['article_subtitle'].apply(strip_html,1)\n",
    "df_unlab['text'] = df_unlab['article_text'].apply(strip_html,1)\n",
    "df_unlab['author'] = df_unlab['author_name'].apply(strip_html,1)\n",
    "\n",
    "suffs = ['.com','.org','.co.uk','.net']\n",
    "df_unlab['source'] = df_unlab['link'].apply(strip_url,1,args=(suffs,))\n",
    "\n",
    "df_unlab.drop(['article_title','article_subtitle','article_text','author_name'], axis =1, inplace=True)\n",
    "\n",
    "df_unlab.reset_index()\n",
    "df_unlab.to_csv(PROJECT_PATH + 'data/cleaned_unlab.csv')\n",
    "df_unlab.to_sql('cleaned',conn,if_exists='replace')\n",
    "conn.commit()\n",
    "\n",
    "df_unlab.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate speech dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now load up the n-grams that were found to be most likely to be found in hatespeech in the paper The \"Hate Speech and Offensive Language\" by Tom Davidson,Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\"\n",
    "\n",
    "Dataset[here](https://data.world/thomasrdavidson/hate-speech-and-offensive-language)..\n",
    "\n",
    "Github [here](https://github.com/t-davidson/hate-speech-and-offensive-language).."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hate_words = pd.read_csv(PROJECT_PATH + 'data/raw_other/hateful_ngrams.csv')\n",
    "df_hate_words.to_sql('hate_nrgams',conn,if_exists='replace')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorize dataset for hatewords\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are going to turn our article text data into vectors to get counts of the occurence of the hateful n-grams in each articles. \n",
    "\n",
    "A few notes:\n",
    "\n",
    "Our effort is to try to bring out dataset in the format that most closely resembles the way in which the authors formatetd their own dataset. To that end, we will use the tokenizer they developed for their dataset to tokenize ours in the same way. We will also generate n-grams in the range of 1-4, since that is the n-gram range used by the authors. \n",
    "\n",
    "Furthermore, we will pass in the \"hateful\" ngrams listed by the authors into our vectorizer so that it only counts the occurences of those n-grams instead of attempting to build a full vocabular of the dataset. The latter is simply too computationlly intensive (even for Colab). At the moment, we are only interested in seeing the occurence of hateful words so this should be fine. \n",
    "\n",
    "After running the CountVectorizer, we will do the following:\n",
    "\n",
    "1. Gather up all the hits for hateful terms found in each document into one dictionary so we can easily look at what ngrams were found for a given article (instead of having to examine all the 178 or so columns)\n",
    "\n",
    "2. Calculate a \"hate score\" This score will be the number of times a hateful ngram occurs in a document with the probability that the presence of that n-gram indicates that there is hatespeech being uttered. These probabilities were generated as the end result of the modeling done by Davidson et all and is found in the df_hate_words frame. \n",
    "\n",
    "3. We will save a whittled down version of the dataframe with the hateful n-grams dataframe to our database, to minimize redundancy and keep the size of the database small. We can later simply merge tables on the index column as needed. We will leave the \"link\" column in there for sanity checks later on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Davidson's custom tokenizer\n",
    "stemmer = PorterStemmer()\n",
    "def tokenize(tweet):\n",
    "    \"\"\"Removes punctuation & excess whitespace, sets to lowercase,\n",
    "    and stems tweets. Returns a list of stemmed tokens.\"\"\"\n",
    "    tweet = \" \".join(re.split(\"[^a-zA-Z]*\", tweet.lower())).strip()\n",
    "    #tokens = re.split(\"[^a-zA-Z]*\", tweet.lower())\n",
    "    tokens = [stemmer.stem(t) for t in tweet.split()]\n",
    "    return tokens\n",
    "\n",
    "# Function to gather and tally hateful n-grams found in a given article and calculate the \"hatescore\" for that article\n",
    "def tally_counts_doc(row):\n",
    "    row2 = row[row > 0]\n",
    "    score = 0\n",
    "    hits = {}\n",
    "    for index,val in row2.items():\n",
    "        hits[index]=val\n",
    "        hit = dict_hateweights[index] * val\n",
    "        score += hit \n",
    "    row['hate_score'] = score\n",
    "    row['hate_hits'] = hits\n",
    "\n",
    "    return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#instantiate and fit the vectorizer\n",
    "unlab_cvect = CountVectorizer(\n",
    "    ngram_range=(1,4),\n",
    "    tokenizer = tokenize,\n",
    "    vocabulary = list(df_hate_words['ngram'])\n",
    ")\n",
    "\n",
    "\n",
    "unlab_vectors = unlab_cvect.fit(df_unlab['text'])\n",
    "unlab_matrix = unlab_vectors.transform(df_unlab['text'])\n",
    "df_vectors = pd.DataFrame(unlab_matrix.todense(),columns=unlab_vectors.get_feature_names())\n",
    "\n",
    "#Gather hate counts and calculate hate scores\n",
    "df_vectors = df_vectors.apply(lambda row:tally_counts_doc(row),axis=1,result_type='expand')\n",
    "df_hatecounts = pd.concat([df_unlab,df_vectors],axis=1)\n",
    "df_hatecounts['hate_hits'] = df_hatecounts['hate_hits'].astype('string')\n",
    "df_hatecounts.to_csv(PROJECT_PATH + 'data/unlab_hatecounts.csv')\n",
    "\n",
    "#Whittle down the vector dataframe to reduce size of DF by curtailing redundant data in the tables and allow for joins\n",
    "df_hatecounts_lite = df_hatecounts.drop([\n",
    "                                         'author',\n",
    "                                         'text',\n",
    "                                         'title',\n",
    "                                         'subtitle',\n",
    "                                         'timestamp',\n",
    "                                         'source',\n",
    "                                         'link'\n",
    "                                         ],axis=1)\n",
    "df_hatecounts_lite.to_sql('hatecounts',conn,if_exists='replace')\n",
    "conn.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utterance Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will try to break apart the articles into sentences to have our own set of \"utterances\" that we can explore and analyse programatically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unlab = pd.read_sql('SELECT * FROM cleaned',conn,index_col='index')\n",
    "\n",
    "df_hatevecs = pd.read_sql('SELECT * FROM hatecounts',conn,index_col='index')\n",
    "df_hatevecs = df_hatevecs.drop(['timestamp','source','link'],axis=1) ## DELETE THIS AFTER RE-RUNNING THE PROCESSING NOTEBOOK \n",
    "\n",
    "df_hatecounts = df_unlab.merge(df_hatevecs,on='index')\n",
    "df_utter_test = df_utter_test[df_utter_test['hate_score'] > 0]\n",
    "df_utter_test = df_hatecounts[['source','link','text','hate_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utters = pd.DataFrame()\n",
    "\n",
    "for index, row in df_utter_test.iterrows():\n",
    "    tokens = sent_tokenize(row['text'])\n",
    "\n",
    "    chunk = pd.DataFrame.from_dict({'sentence':tokens})\n",
    "    chunk['source'] = row['source']\n",
    "    chunk['link'] = row['link']\n",
    "    chunk['article_index'] = index\n",
    "\n",
    "    df_utters = pd.concat([df_utters,chunk])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "sent_cvect = CountVectorizer(\n",
    "    ngram_range=(1,4),\n",
    "    tokenizer = tokenize,\n",
    "    vocabulary = list(df_hate_words.index)\n",
    ")\n",
    "\n",
    "\n",
    "sent_vectors = sent_cvect.fit(df_utters['sentence'])\n",
    "sent_matrix = sent_vectors.transform(df_utters['sentence'])\n",
    "df_svectors = pd.DataFrame(sent_matrix.todense(),columns=sent_vectors.get_feature_names())\n",
    "\n",
    "#Gather hate counts and calculate hate scores\n",
    "df_svectors = df_svectors.apply(lambda row:tally_counts_doc(row),axis=1,result_type='expand')\n",
    "df_shatecounts = pd.concat([df_utters,df_svectors],axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
