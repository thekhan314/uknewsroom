{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "visible-retention",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-breakdown",
   "metadata": {},
   "source": [
    "We have previously loaded up data from The \"Hate Speech and Offensive Language\" by project Tom Davidson,Dana Warmsley, Michael Macy, and Ingmar Weber. 2017. \"Automated Hate Speech Detection and the Problem of Offensive Language.\" We got the n-grams whose presence Davidson et al found to make it highly probable that the speech in question was hate speech. We then processed out data to count the occurences of any of these particular n-grams in each of our articles. \n",
    "\n",
    "Lets no proceed to take a closer look on the distribution of these nrgams in our own dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "centered-gibraltar",
   "metadata": {},
   "source": [
    "# Count of hateful n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-forward",
   "metadata": {},
   "source": [
    "First and foremost, I want to tall up the total number of each of the n-grams that were found anywhere in our dataset, regardless of where they were found. This is a simple matter of adding up the columns for each n-gram generated by the CountVectorizer algorithm. We will then generate a bar plot to see which n-grams occur most often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-cornwall",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatevecs = df_hatevecs.drop(['hate_score','hate_hits'],axis=1)\n",
    "df_counts = df_hatevecs.sum(axis=0)\n",
    "df_counts.sort_values(inplace=True,ascending = False)\n",
    "df_counts = df_counts[df_counts > 0]\n",
    "df_counts.plot(kind='barh',figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "classical-sleeve",
   "metadata": {},
   "source": [
    "Interesting! This makes sense. We see that the n-grams that occure most often are in fact what we would intuitively expect to see most often in hate speech. I find it particularly interesting that \"blame the\" is one of the highest occuring n-grams and this makes compelete sense, since peoples need to scapegoat and assign blame is very often at the root of their biases. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rough-dominant",
   "metadata": {},
   "source": [
    "# Distribution of 'hate score'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "significant-fellow",
   "metadata": {},
   "source": [
    "Now I want to take a closer look at the hate scores. Recall that each article is being assigned a hate score calculated by multiplying the count of each of the hateful n-grams found in it by the probability Davidson and all found of such n-grams indicating hate speech. This way, n-grams that are less likely to indicate hate speech will contribute less towards the articles hate score and vice versa. \n",
    "\n",
    "Lets see how the hate scores are distributed across the articles in a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cloudy-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatescore = df_hatecounts[df_hatecounts['hate_score'] > 0]\n",
    "df_hatescore['hate_score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "isolated-factory",
   "metadata": {},
   "source": [
    "Looks like most of the articles have a hate score of '1', with the fewer and few articles having higher number of hateful n-grams. This seems about what we would expect. \n",
    "\n",
    "Lets take a closer look at articles with a higher hatescore, anything above 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "actual-empire",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hatescore2 = df_hatescore[df_hatescore['hate_score'] > 2 ]\n",
    "df_hatescore2['hate_score'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-overall",
   "metadata": {},
   "source": [
    "# Average hate score per source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-employee",
   "metadata": {},
   "source": [
    "Im now curious to see what the average hatescore is for each news source. Are some sources more likely to contain hateful n-grams than others?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_source_scores = df_hatecounts[['source','hate_score']].groupby('source').mean()\n",
    "df_source_scores.sort_values('hate_score',inplace=True)\n",
    "df_source_scores.plot(kind='barh',figsize=(10,15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hollow-confidentiality",
   "metadata": {},
   "source": [
    "Just eyeballing this it definitely seems to corroborate alot of \"conventional wisdom\" about hate speech. Generally, hate speech is seen as being a problem associated more with the conservative end of the political spectrum (my apologies if you disagree with this generalization). Sure enough, the two highest hate scores are for \"the American conservative\" and \"the American thinker\" , both sources that are solidly towards the right of the political spectrum.\n",
    "Ofcourse I again want to reiterate , as Terry pointed out, that the presence of terms associated with hate speech do not necessarily indicate that hate speech is being promoted.  An excellent case in point is \"Vox\", which  has the third highest score. But Vox does a lot of work doing like in depth, embedded, gonzo style journalistic pieces on hate groups like neo Nazis and KKK etc, so you would expect their articles to reference alot of hate speech. But most likely, the hits found on vox will be related to articles discussing or describing hateful speech and conduct and not necessarily articulating it as their own stance. You also see nytimes and nbc news up there, to further highlight the fact that simple occurrence of hateful terminology is not sufficient to arrive at any kind of conclusion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disabled-procedure",
   "metadata": {},
   "source": [
    "# Utterance level analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utter = pd.read_sql('SELECT * FROM utterances',conn,index_col='index')\n",
    "df_unlab = pd.read_sql('SELECT * FROM cleaned',conn,index_col='index')\n",
    "df_article_index = df_unlab.reset_index()[['text','source','link','index']].drop_duplicates(subset='link')\n",
    "df_article_index.head()\n",
    "\n",
    "df_utter_redux = df_utter.merge(df_article_index,how='inner',left_on='article_index',right_on='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marine-touch",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utter_redux['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reasonable-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_utter_redux.head(70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
